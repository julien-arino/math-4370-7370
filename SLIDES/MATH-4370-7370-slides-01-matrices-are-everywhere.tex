\documentclass[aspectratio=169]{beamer}

%\documentclass[handout]{beamer}
%% To make 4 per page
%\usepackage{pgfpages}
%\mode<handout>{\setbeamercolor{background canvas}{bg=white}}
%\pgfpagesuselayout{4 on 1}[letterpaper,landscape]%,border shrink=5mm]

\input{slides_setup_nonLightBoard_whiteBG.tex}



\title{Matrices are \emph{everywhere}}
\author{Julien Arino}
\date{Fall 2023}

\begin{document}

% The title page
\begin{frame}[noframenumbering,plain]
	\begin{tikzpicture}[remember picture,overlay]
		\node[above right,inner sep=0pt] at (current page.south west)
		{
			\includegraphics[width=\paperwidth]{title-page-picture.png}
		};
	\end{tikzpicture}	
	\titlepage
\end{frame}
\addtocounter{page}{-1}
  
  
\begin{frame}{Outline}
	\tableofcontents[hideallsubsections]
\end{frame}
\addtocounter{page}{-1}


%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
We give a few examples that illustrate how ubiquitous matrices are in mathematics
\vfill
In the process, we introduce some concepts that are used later
\vfill
However, precise definitions are given in subsequent chapter; here concepts are introduced with very few explanations
\end{frame}

%\begin{frame}[noframenumbering,plain]{OUTLINE}
%\tableofcontents
%\end{frame}


%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\section{Linear systems of difference equations}
\label{sec:linear_DE}
\begin{frame}{Linear systems of difference equations}
Let $x(t)\in \IR^n$ be a \defword{state variable} and $t\in \IN$ be an \defword{independent variable}, typically thought of as \defword{time}. Let $A\in \mathcal{M}_n(\IR)$. 
An \defword{autonomous homogeneous linear system of difference equations} is a sequence defined by
\begin{subequations}\label{sys:DE}
\begin{align}
x(t+1) &= Ax(t) \label{sys:DE_dx}\\
x(0) &=x_0\in \IR^n, \label{sys:DE_IC}
\end{align}	
\end{subequations}
where $x_0$ is called the \defword{initial condition} (IC)
\vfill
(Autonomous: $A$ is a constant that does not depend on $t$. Homogeneous: the system is not of the form $x(t+1)=Ax(t)+b$)
\end{frame}

\begin{frame}
Given the initial condition $x(0)=x_0$, we have from \eqref{sys:DE_dx} that
\begin{align*}
 &x(1)= Ax(0)=Ax_0\\
 & x(2)= Ax(1)= AAx_0=A^2x_0 \\
 &x(3)=Ax(2)=AA^2x_0=A^3x_0
\end{align*}
\vfill
By induction,
\[
x(t)=A^tx_0
\] 
for all $t\in\IN$.
Thus the behaviour of $x(t)$ as $t\to\infty$ depends on $A^t$
\vfill
In order to understand what this behaviour could be, the following two questions should be answered
\begin{enumerate}
	\item What is (if it exists) $\lim_{t\to \infty} x(t)$? 
	\item What is (if it exists) $\lim_{t \to \infty} A^t$? 
\end{enumerate}
\end{frame}

\begin{frame}
Let $v$ be an eigenvector associated to the eigenvalue $\lambda$ of $A$, \ie, $\lambda$ be such that $v\neq 0$ satisfies $Av=\lambda v$. Then we have
 \begin{align*}
 A^2v &= A(Av) \\
 &=A(\lambda v) \\
 &=\lambda Av \\
 &=\lambda^2 v
 \end{align*}
\ie, $v$ is also an eigenvector of $A^2$ and is associated to the eigenvalue $\lambda^2$.
By induction
\[
A^kv=\lambda^k v
\]
\ie, $v$ is an eigenvector of the matrix $A^k$ associated to the eigenvalue $\lambda^k$. $A^kv$ is a vector in $\IF^n$; we can thus take its norm $\|A^kv\|$, where $\|\cdot\|$ is some norm on $\IF^n$. It follows that if $|\lambda|<1$, then $\|A^kv\|=|\lambda|^k\|v\|$ goes to zero as $k\to\infty$
\end{frame}


\begin{frame}
\begin{theorem}\label{th:TFAE_DE}
Let $A\in\M_n(\IR)$, consider the map $Ax$. TFAE:
\begin{enumerate}
\item There exists a norm $\|\;\|_\alpha$ on $\IR^n$ and a constant $0<\mu<1$ such that for any $x\in\IR^n$, the iterates satisfy, for all $k\geq 0$
\[
\|A^kx\|_\alpha\leq \mu^k\|x\|_\alpha
\]
\item For any norm $\|\;\|_\beta$ on $\IR^n$, there exists constants $0<\mu<1$ and $C\geq 1$ such that all $x\in\IR^n$ and all $k\geq 0$
\[
\|Ax\|_\beta\leq C\mu^k\|x\|_\beta
\]
\item All the eigenvalues $\lambda$ of $A$ satisfy $|\lambda|<1$
\end{enumerate}
\end{theorem}
\end{frame}

\begin{frame}
To use the result above, we need to ensure all eigenvalues lie inside the unit disk in $\IC$
\vfill
For certain classes of matrices, this can be achieved without explicitly computing the eigenvalues
\vfill

A linear map corresponding to a matrix with all eigenvalues of modulus less than 1 is a \defword{linear contraction}, with the origin a \defword{linear sink} or \defword{attracting fixed point}. If all eigenvalues have modulus larger than 1, then the map induced by $A$ is a \defword{linear expansion}, and the origin is a \defword{linear source} or \defword{repelling fixed point}.
The map $Ax$ is a \defword{hyperbolic linear map} if all eigenvalues of $A$ have modulus different of 1
\end{frame}

\begin{frame}
\begin{definition}
Let $\M_n$ be the set of square $n\times n$ matrices. For $A\in\M_n$, denote $\sigma(A)$ the set of its eigenvalues, \ie,
\[
\sigma(A)=\left\{
\lambda\in\IC; \exists v\neq 0,\; Av=\lambda v
\right\}
\]
which we call the \defword{spectrum} of $A$. We call \defword{spectral radius} of $A$ the real number
\[
\rho(A)=\max_{\lambda\in\sigma(A)}\{|\lambda|\}
\]
\end{definition}
%%
\vfill
%%
\begin{theorem}\label{th:matrix_everywhere_rho_l_1_goes0}
If $\rho(A)<1$, then $\lim\limits_{t\to \infty} x(t)=0$ for system \eqref{sys:DE}
\end{theorem}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example -- Leslie matrices}
Leslie matrices arise when considering the \emph{age-structured} dynamics of populations reproducing every year, such as most fish
\vfill
Assume $n$ is the maximum age observed for that species. Time $t$ is taken in years and is \emph{discrete}.
Let $x(t)=(x_1(t),\ldots,x_n(t))^T$ be the vector of distribution of the population in ages, \ie, $x_i(t)$ is the population of fish of age $a$ between $i-1$ and $i$ at time $t$
\vfill


A proportion $s_i\in[0,1]$ of individuals of age $i-1\leq a<i$ survive to the next year. As years progress at the same rate as age, this means that $x_{i+1}(t+1) = s_i x_i(t)$
\vfill
When individuals reproduce, they give birth to $f_i$ individuals in the first age class, \ie, birth in the first age class takes the form
\[
x_1(t+1)=f_1x_1(t)+f_2x_2(t)+\cdots+f_nx_n(t)
\]
\end{frame}

\begin{frame}{Population growth model with a Leslie matrix}
\begin{align*}
 x_1(t+1) &= f_1x_1(t)+f_2x_2(t)+\cdots+f_nx_n(t) \\
 x_2(t+1) &= s_1x_1(t) \\
 x_3(t+1) &= s_2x_2(t) \\
 &\vdots \\
 x_n(t+1) &= s_{n-1}x_{n-1}(t)
\end{align*}
\end{frame}

\begin{frame}
Suppose that on the first year counts are taken, we observe an initial age distribution $x(0)=(x_1(0),\ldots,x_n(0))^T$
\vfill
Assume the survival rates $s_i$ and fecundity constants $f_i$ are known
\vfill
How can we expect the population to evolve over the course of time? In particular, is the species likely to survive or will it become extinct?
\end{frame}

\begin{frame}
Write the system in vector form
\begin{equation}\label{eq:Leslie_long}
\begin{pmatrix}
x_1(t+1) \\ x_2(t+1) \\ x_3(t+1) \\ \vdots \\ x_n(t+1)
\end{pmatrix}
=
\begin{pmatrix}
f_1 & f_2 & f_3 & \cdots & f_{n-1} & f_n \\
s_1 & 0 & 0 & \cdots & 0 & 0 \\
0 & s_2 & 0 & \cdots & 0 & 0 \\
\vdots && \ddots &&& \vdots \\
0 &&&&s_{n-1} & 0
\end{pmatrix}
\begin{pmatrix}
x_1(t) \\ x_2(t) \\ x_3(t) \\ \vdots \\ x_n(t)
\end{pmatrix}
\end{equation}
which we summarize as
\begin{equation}\label{eq:Leslie_short}
x(t+1)=Lx(t)
\end{equation}
with $L$ the matrix in \eqref{eq:Leslie_long}, \ie, a \defword{nonnegative} matrix with only the first row and the first sub-diagonal nonzero
\end{frame}


\begin{frame}
\[
L= \begin{pmatrix}
f_1 & f_2 & f_3 & \cdots & f_{n-1} & f_n \\
s_1 & 0 & 0 & \cdots & 0 & 0 \\
0 & s_2 & 0 & \cdots & 0 & 0 \\
\vdots && \ddots &&& \vdots \\
0 &&&&s_{n-1} & 0
\end{pmatrix}
\]
\vfill
By Theorem~\ref{th:matrix_everywhere_rho_l_1_goes0}, if $\rho(L)<1$ then $x(t)\to 0$ at $t\to\infty$, so in this case, the population would become extinct
\vfill
By the discussion earlier, if $\rho(L)=1$, then $\|x(t)\|$ (the total population) stays constant
\vfill
If $\rho(L)>1$, the population increases
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\section{Ordinary differential equation}

\begin{frame}{ODE}
An \defword{ordinary differential equation} (ODE) is an equation of the form 
\begin{equation}\label{eq:nonlinear_ODE}
\dfrac{d}{dt}x(t)= f(x(t))
\end{equation}
where $x(t)\in \IR^n$ is a \emph{function} and $f:\IR^n \to \IR^n$
\vfill
An \defword{initial value problem} (IVP) is the consideration of \eqref{eq:nonlinear_ODE} together with an initial condition $x(t_0)=x_0\in\IR^n$
\vfill
A solution to \eqref{eq:nonlinear_ODE} is a function $\phi(t)$ that satisfies \eqref{eq:nonlinear_ODE}. A solution to the IVP with $x(t_0)=x_0$ associated to \eqref{eq:nonlinear_ODE} is, among all solutions to \eqref{eq:nonlinear_ODE}, the one (typically the only one) that additionally satisfies the initial condition, \ie, such that $\phi(t_0)=x_0$
\end{frame}

\begin{frame}{Linear systems}
Consider the autonomous linear system
\begin{equation}\label{eq:Lin_EDO_cc_hom}
x'(t)=Ax(t)
\end{equation}
where $A\in\M_n$ is a constant
\vfill
Let $\sigma(A)=\{\lambda_1,\ldots,\lambda_n\}$ be the spectrum of $A$.
Let $w_j=u_j+iv_j$ be a generalized eigenvector of $A$ corresponding to an eigenvalue $\lambda_j=a_j+ib_j$, with $v_j=0$ if $b_j=0$, and 
\[
B=\{u_1,\ldots,u_k,u_{k+1},v_{k+1},\ldots,u_m,v_m\}
\]
be a basis of $\mathbb{R}^n$, with $n=2m-k$, where $k$ is the number of real eigenvalues in $\sigma(A)$
\end{frame}

\begin{frame}
\begin{definition}[Stable, unstable and center subspaces]
The \defword{stable}, \defword{unstable} and \defword{center} subspaces of the linear system \eqref{eq:Lin_EDO_cc_hom} are given, respectively, by
\[
E^s=\textrm{Span}\{u_j,v_j:a_j<0\},
\]
\[
E^u=\textrm{Span}\{u_j,v_j:a_j>0\}
\]
and
\[
E^c=\textrm{Span}\{u_j,v_j:a_j=0\}
\]
\end{definition}
\end{frame}

\begin{frame}
\begin{definition}
The mapping $e^{At}:\mathbb{R}^n\to\mathbb{R}^n$ is the \defword{flow} of the linear system \eqref{eq:Lin_EDO_cc_hom}
\end{definition}
The term \defword{flow} is used since $e^{At}$ describes the motion of points $x_0\in\mathbb{R}^n$ along trajectories of \eqref{eq:Lin_EDO_cc_hom}
%%
\vfill
%%
\begin{definition}
If all eigenvalues of $A$ have nonzero real part, \ie, if $E^c=\emptyset$, then the flow $e^{At}$ of system \eqref{eq:Lin_EDO_cc_hom} is a \defword{hyperbolic flow} and the system \eqref{eq:Lin_EDO_cc_hom} is a \defword{hyperbolic linear system}
\end{definition}
%%
\vfill
%%
\begin{definition}
A subspace $E\subset\mathbb{R}^n$ is \defword{invariant with respect to the flow} $e^{At}$, or \defword{invariant under the flow} of \eqref{eq:Lin_EDO_cc_hom}, if $e^{At}E\subset E$ for all $t\in\mathbb{R}$
\end{definition}
\end{frame}

\begin{frame}
\begin{theorem}
Let $E$ be the generalized eigenspace of $A$ associated to the eigenvalue $\lambda$. Then $AE\subset E$
\end{theorem}
%%
\vfill
%%
\begin{theorem}
Let $A\in\mathcal{M}_n(\mathbb{R})$. Then
\[
\mathbb{R}^n=E^s\oplus E^u\oplus E^c
\]
Furthermore, if the matrix $A$ is the matrix of the linear autonomous system \eqref{eq:Lin_EDO_cc_hom}, then $E^s$, $E^u$ and $E^c$ are invariant under the flow of \eqref{eq:Lin_EDO_cc_hom}, \ie, let
$x_0\in E^S$, $y_0\in E^C$ and $z_0\in E^U$, then $e^{At}x_0\in E^S$,
$e^{At}y_0\in E^C$ and $e^{At}z_0\in E^U$
\end{theorem}
\end{frame}



\begin{frame}{Nonlinear systems of ODE}
There is no general theory allowing to obtain explicit solutions of a nonlinear IVP
\vfill
Instead of seeking explicit solutions, we use \defword{qualitative analysis}, which uses analysis to establish properties of the solutions without needing to actually find their explicit expression
\end{frame}


\begin{frame}
Suppose you are given a system of ordinary differential equations
\begin{equation}\label{sys:ODE}
x'=f(x),
\end{equation}
where $x\in\IR^n$ and $f:\IR^n\to\IR^n$ is $C^1$. A standard step when studying \eqref{sys:ODE} \defword{qualitatively} is to seek \defword{equilibria} of \eqref{sys:ODE}, \ie, points $x^\star\in\IR^n$ such that
\begin{equation}\label{eq:equilibrium}
f(x^\star)=0.
\end{equation}
At such a point, $x'=0$, meaning that system \eqref{sys:ODE} is at rest. If you were to consider solutions to \eqref{sys:ODE} with an initial condition $x(0)=x^\star$, then there would hold that $x(t)=x^\star$ for all $t\geq 0$
\vfill
What would happen if instead of starting at $x^\star$, you were to choose an initial condition $x(0)$ close to but distinct from $x^\star$?
\end{frame}

\begin{frame}{Simplified version of Hartman-Grobman}
\begin{definition}
An equilibrium point $x^\star$ is \textbf{hyperbolic}\index{hyperbolic equilibrium point} if the Jacobian matrix $Df$ of \eqref{sys:ODE} evaluated at $x^\star$, denoted $Df(x^\star)$, has no eigenvalues with zero real part, \ie, is invertible
\end{definition}
\vfill
\begin{theorem}[Hartman-Grobman]\label{th:HartmanGrobman}
Let $x^\star$ be a hyperbolic equilibrium point of \eqref{sys:ODE}.
Then in some neighbourhood $\mathcal{N}(x^\star)$ of $x^\star$, the flow of \eqref{sys:ODE} is topologically equivalent to the flow of the linear system 
\begin{equation}\label{sys:linearized}
x'=Df(x^\star)(x-x^\star)
\end{equation} 
where $Df(x^\star)$ is the Jacobian matrix $Df$ of $f$ evaluated at $x^\star$
\end{theorem}
\end{frame}

\begin{frame}
\begin{theorem}[Stable manifold theorem]\label{th:stable_manifold}
Let $E$ be an open subset of $\IR^n$ containing the origin, let $f\in
C^1(E)$, and let $\phi_t$ be the flow of the nonlinear system
\eqref{sys:ODE}. Suppose that $f(0)=0$ and that $Df(0)$ has $k$
eigenvalues with negative real part and $n-k$ eigenvalues with
positive real part. Then there exists a $k$-dimensional differentiable
manifold $S$ tangent to the stable subspace $E^s$ of the linear system
(\ref{sys:linearized}) at 0 such that for all $t\geq 0$,
$\phi_t(S)\subset S$ and for all $x_0\in S$
\[
\lim_{t\to\infty} \phi_t(x_0)=0
\]
and there exists an $(n-k)$-dimensional differentiable manifold $U$
tangent to the unstable subspace $E^u$ of (\ref{sys:linearized}) at 0
such that for all $t\leq 0$, $\phi_t(U)\subset U$ and for all $x_0\in
U$
\[
\lim_{t\to-\infty}\phi_t(x_0)=0
\]
\end{theorem}
%%
\end{frame}
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
\begin{frame}{Example -- A chemostat model}
System of 2 nonlinear DE modelling a biological device called a \defword{chemostat}
\begin{subequations}
\label{sys:chemostat}
\begin{align}
\frac{dS}{dt} &= D\left(S^0-S\right)-\mu(S)x \label{sys:chemostatS} \\
\frac{dx}{dt} &= (\mu(S)-D)x \label{sys:chemostatx}
\end{align}
\end{subequations}
%%
\vfill
%%
Parameters $S^0$ and $D$, respectively the \defword{input concentration} and the \defword{dilution} rate, are real and positive.
The function $\mu$ is the \defword{growth} function. It is generally
assumed to satisfy $\mu(0)=0$, $\mu'>0$ and $\mu''<0$
\end{frame}

\begin{frame}
One can verify that the positive quadrant is
positively invariant under the flow of (\ref{sys:chemostat}),
\ie, that for $S(0)\geq 0$ and $x(0)\geq 0$, solutions remain nonnegative for all positive times, and similar properties
\vfill
But since we are here only interested in applications of the stable
manifold theorem, we proceed to a very crude analysis, and will not
deal with this point
\end{frame}

\begin{frame}
Write the system in vector form as
\[
\xi'=f(\xi),
\]
with $\xi=(S,x)^T$ and
\[
f(\xi)=
\begin{pmatrix}
D(S^0-S)-\mu(S)x \\
(\mu(S)-D)x
\end{pmatrix}
\]
EP of the system are found by solving $f(\xi)=0$. We find one situated on one of the boundaries of the positive quadrant
\[
\xi_T^\star=(S^\star_T,x^\star_T)=\left(S^0,0\right)
\]
and the second one in the interior of $\IR_+^2$,
\[
\xi_I^\star=(S^\star,x^\star)=\left(\lambda,S^0-\lambda\right)
\]
where $\lambda$ is such that $\mu(\lambda)=D$. Note that this implies
that if $\lambda\geq S^0$, $\xi_T^\star$ is the only equilibrium of the
system since in that case, $\xi_I^\star\not\geq 0$, which is not biologically realistic
\end{frame}

\begin{frame}
At an arbitrary point $\xi=(S,x)$, the Jacobian matrix of \eqref{sys:chemostat} is given by
\[
Df(\xi)=\left(
\begin{matrix}
-D-\mu'(S)x & -\mu(S) \\
\mu'(S)x & \mu(S)-D
\end{matrix}
\right)
\]
%%
\vfill
%%
Thus, at the trivial equilibrium $\xi_T^\star$
\[
Df(\xi_T^\star)=
\left(
\begin{matrix}
-D & -\mu(S^0) \\
0 & \mu(S^0)-D
\end{matrix}
\right)
\]
%%
\vfill
%%
We have two eigenvalues, $-D$ and $\mu(S^0)-D$.
Since $-D<0$, we focus on the eigenvalue $\mu(S^0)-D$
\end{frame}

\begin{frame}
Assume that
$\mu(S^0)-D<0$. This implies that $\xi_T^\star$ is the only
equilibrium, since $\xi_I^\star$ is not feasible
if $\lambda>S^0$
\vfill
System has dimensionality 2 and $Df(\xi_T^\star)$
has two negative eigenvalues $\implies$ the stable manifold theorem
(Theorem~\ref{th:stable_manifold}) states that there exists a
2-dimensional differentiable manifold $\mathcal{M}$ such that
\begin{itemize}
\item $\phi_t(\mathcal{M})\subset\mathcal{M}$,
\item for all $\xi_0\in\mathcal{M}$,
$\lim_{t\to\infty}\phi_t(\xi_0)=\xi_T^\star$.
\item At $\xi_T^\star$, $\mathcal{M}$ is tangent to the stable subspace
$E^S$ of the linearized system $\xi'=Df(\xi_T^\star)(\xi-\xi_T^\star)$.
\end{itemize}
Since there are no eigenvalues with positive real part, there does not
exist an unstable manifold in this case 
\end{frame}

\begin{frame}
Let us now characterize the
nature of the stable subspace $E^S$. It is obtained by studying the
linear system
\begin{align}
\xi' &= Df(\xi_T^\star)(\xi-\xi_T^\star) \nonumber \\
&= \left(
\begin{matrix}
-D & -\mu(S^0) \\
0 & \mu(S^0)-D
\end{matrix}
\right)
\left(
\begin{matrix}
S-S^0 \\
x
\end{matrix}
\right) \nonumber \\
&= \left(
\begin{matrix}
-D(S-S^0)-\mu(S^0)x \\
(\mu(S^0)-D)x
\end{matrix}
\right)
\end{align}
Of course, the Jacobian matrix associated to this system is the same
as that of the nonlinear system (at $\xi_T^\star$). Associated to the
eigenvalue $-D$ is the eigenvector $v_1=(1,0)^T$, to $\mu(S^0)-D$ is
$v_2=(-1,1)^T$
\end{frame}

\begin{frame} 
The stable subspace is thus given by $\Span(v_1,v_2)$, \ie,
the whole of $\IR^2$
\vfill
In fact, the stable manifold of $\xi_T^\star$ is the whole positive
quadrant, since all solutions limit to this equilibrium 
\vfill
The same type of analysis can be conducted at the interior equilibrium $\xi_I^\star$. It is a little harder in this case, since $x^\star>0$ there and therefore the Jacobian matrix $Df(\xi_I^\star)$ does not have the same upper triangular structure as $Df(\xi_T^\star)$
\end{frame}

%%%%%%%%%%%%%%
%%%%%%%%%%%%%%
%%%%%%%%%%%%%%
%%%%%%%%%%%%%%
\section{Linear systems of ODE -- Brief theory}


\frame{\frametitle{Linear ODEs}
\begin{definition}[Linear ODE]
A \emph{linear} ODE is a differential equation taking the form
\begin{equation}\label{sys:linear_general}
\frac{d}{dt}x=A(t)x+B(t),\tag{LNH}
\end{equation}
where $A(t)\in\mathcal{M}_n(\IR)$ with continuous entries, $B(t)\in\IR^n$ with real valued, continuous coefficients, and $x\in\IR^n$. The associated IVP takes the form 
\begin{equation}\label{sys:IVP_linear_general}
\begin{aligned}
\frac{d}{dt}x &= A(t)x+B(t) \\
x(t_0)&=x_0.
\end{aligned}
\end{equation}
\end{definition}
}

\frame{\frametitle{Types of systems}
\begin{itemize}
\item $x'=A(t)x+B(t)$ is linear nonautonomous ($A(t)$ depends on $t$) nonhomogeneous (also called \emph{affine} system).
\item $x'=A(t)x$ is linear nonautonomous homogeneous.
\item $x'=Ax+B$, that is, $A(t)\equiv A$ and $B(t)\equiv B$, is linear autonomous nonhomogeneous (or affine autonomous).
\item $x'=Ax$ is linear autonomous homogeneous.
\end{itemize}
}


\frame{\frametitle{Existence and uniqueness of solutions}
\begin{theorem}[Existence and Uniqueness]
Solutions to \eqref{sys:IVP_linear_general} exist and are unique on the whole interval over which $A$ and $B$ are continuous.

In particular, if $A,B$ are constant, then solutions exist on $\IR$.
\end{theorem}
}

\frame{\frametitle{Autonomous linear systems}
Consider the autonomous affine system
\begin{equation}\label{sys:affine_auton}
\frac{d}{dt}x=Ax+B,\tag{A}
\end{equation}
and the associated homogeneous autonomous system
\begin{equation}\label{sys:lin_auton}
\frac{d}{dt}x=Ax.\tag{L}
\end{equation}
}

\frame{\frametitle{Exponential of a matrix}
\begin{definition}[Matrix exponential]
Let $A\in\M_n(\IF)$ with $\IF=\IR$ or $\IC$. The \emph{exponential} of $A$, denoted $e^{At}$, is a matrix in $\M_n(\IF)$, defined by
\[
e^{At}=\II +\sum_{k=1}^\infty \frac{t^k}{k!}A^k,
\]
where $\II$ is the identity matrix in $\M_n(\IF)$.
\end{definition}
}

\frame{\frametitle{Properties of the matrix exponential}
\begin{itemize}
\item $e^{At_1}e^{At_2}=e^{A(t_1+t_2)}$ for all $t_1,t_2\in\IR$.
1\item $Ae^{At}=e^{At}A$ for all $t\in\IR$.
\item $(e^{At})^{-1}=e^{-At}$ for all $t\in\IR$.
\item The unique solution $\phi$ of \eqref{sys:lin_auton} with $\phi(t_0)=x_0$ is given by
\[
\phi(t)=e^{A(t-t_0)}x_0.
\]
\end{itemize}
}

\frame{\frametitle{Computing the matrix exponential}
Let $P$ be a nonsingular matrix in $\M_n(\IR)$. We transform the IVP
\begin{equation}\label{IVP:lin_auton}
\begin{aligned}
\frac{d}{dt}x &= Ax\\
x(t_0)&=x_0
\end{aligned}\tag{LIVP}
\end{equation}
using the transformation $x=Py$ or $y=P^{-1}x$.
\vfill
The dynamics of $y$ is $y'=P^{-1}APy$.
% \begin{align*}
% y' &= (P^{-1}x)' \\
% &= P^{-1}x' \\
% &= P^{-1}Ax \\
% &= P^{-1}APy
% \end{align*}
The initial condition is $y_0=P^{-1}x_0$.
}

\frame{
We have thus transformed IVP \eqref{IVP:lin_auton} into
\begin{equation}\label{IVP:lin_auton2}
\begin{aligned}
\frac{d}{dt}y &= P^{-1}APy\\
y(t_0)&=P^{-1}x_0
\end{aligned}\tag{L\_IVP\_y}
\end{equation}
From the earlier result, we then know that the solution of \eqref{IVP:lin_auton2} is given by
\[
\psi(t)=e^{P^{-1}AP(t-t_0)}P^{-1}x_0,
\]
and since $x=Py$, the solution to \eqref{IVP:lin_auton} is given by
\[
\phi(t)=Pe^{P^{-1}AP(t-t_0)}P^{-1}x_0.
\]
So everything depends on $P^{-1}AP$.
}

\frame{\frametitle{The cases}
\begin{itemize}
\item $P^{-1}AP$ is diagonal,
the solution to \eqref{IVP:lin_auton} is given by
\[
\phi(t)=P
\begin{pmatrix}
e^{\lambda_1t} & & 0 \\
& \ddots & \\
0 & & e^{\lambda_nt}
\end{pmatrix}
P^{-1}x_0.
\]
\item $P^{-1}AP$ is not diagonal, then use Jordan form (slightly more complicated).
\end{itemize}
}

\frame{
\begin{theorem}
For all $(t_0,x_0)\in\IR\times\IR^n$, there is a unique solution $x(t)$ to \eqref{IVP:lin_auton} defined for all $t\in\IR$. Each coordinate function of $x(t)$ is a linear combination of functions of the form
\[
t^ke^{\alpha t}\cos(\beta t)\quad\textrm{and}\quad t^ke^{\alpha t}\sin(\beta t)
\]
where $\alpha+i\beta$ is an eigenvalue of $A$ and $k$ is less than the algebraic multiplicity of the eigenvalue.
\end{theorem}
}

\frame{\frametitle{Generalized eigenvectors, nilpotent matrix}
\begin{definition}[Generalized eigenvectors]
Let $A\in\M_r(\IR)$. Suppose $\lambda$ is an eigenvalue of $A$ with multiplicity $m\leq n$. Then, for $k=1,\ldots,m$, any nonzero solution $v$ of
\[
(A-\lambda\II)^kv=0
\]
is called a \emph{generalized eigenvector} of $A$.
\end{definition}
\begin{definition}[Nilpotent matrix]
Let $A\in\M_n(\IR)$. $A$ is \emph{nilpotent} (of order $k$) if $A^{j}\neq 0$ for $j=1,\ldots,k-1$, and $A^k=0$.
\end{definition}
}

\frame{\frametitle{Jordan normal form}
\begin{theorem}[Jordan normal form]
Let $A\in\M_n(\IR)$ have eigenvalues $\lambda_1,\ldots,\lambda_n$, repeated according to their multiplicities. 
\begin{itemize}
\item 
Then there exists a basis of generalized eigenvectors for $\IR^n$. 
\item 
And if $\{v_1,\ldots,v_n\}$ is any basis of generalized eigenvectors for $\IR^n$, then the matrix $P=[v_1\cdots v_n]$ is invertible, and $A$ can be written as
\[
A=S+N,
\]
where
\[
P^{-1}SP=\diag(\lambda_j),
\]
the matrix $N=A-S$ is nilpotent of order $k\leq n$, and $S$ and $N$ commute, i.e., $SN=NS$.
\end{itemize}
\end{theorem}
}

\frame{
\begin{theorem}
Under conditions of the Jordan normal form Theorem, the linear system $x'=Ax$ with initial condition $x(0)=x_0$, has solution
\[
x(t)=P\diag\left(e^{\lambda_j t}\right)P^{-1}\left(\II+Nt+\cdots\frac{t^k}{k!}N^k\right) x_0.
\]
\end{theorem}
The result is particularly easy to apply in the following case.
\begin{theorem}[Case of an eigenvalue of multiplicity $n$]
Suppose that $\lambda$ is an eigenvalue of multiplicity $n$ of $A\in\M_n(\IR)$. Then $S=\diag(\lambda)$, and the solution of $x'=Ax$ with initial value $x_0$ is given by
\[
x(t)=e^{\lambda t}\left(\II+Nt+\cdots\frac{t^k}{k!}N^k\right) x_0.
\]
\end{theorem}
In the simplified case, we do not need the matrix $P$ (the basis of generalized eigenvectors).
}

% \frame{\frametitle{A variation of constants formula}
% \begin{theorem}[Variation of constants formula]
% Consider the IVP
% \begin{subequations}\label{sys:lin_nonlin}
% \begin{align}
% x' &= Ax+B(t) \\
% x(t_0) &= x_0,
% \end{align}
% \end{subequations}
% where $B:\IR\to\IR^n$ a smooth function on $\IR$, and let $e^{A(t-t_0)}$ be matrix exponential associated to the homogeneous system $x'=Ax$. Then the solution $\phi$ of \eqref{sys:lin_nonlin} is given by
% \begin{equation}
% \phi(t)=e^{A(t-t_0)}x_0 + \int_{t_0}^t e^{A(t-s)}B(s)ds.
% \end{equation}
% \end{theorem}
% }

%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\section{Discrete-time Markov chains}
\begin{frame}{Discrete-time Markov chain}
A \defword{discrete-time Markov chain} is a stochastic process
\vfill
Consider a system with $n$ \defword{states} denoted $S_1,\ldots,S_n$. The system starts in a given state. Every time step, it switches to a different state. (Transition from one state to itself is also allowed)
\vfill
We assume that the system is \emph{stochastic}, \ie, that the transitions happen at random. In discrete-time Markov chains, the instants at which transitions occur (or not) are in a discrete set, typically rescaled to be $\IN$
\vfill
(In \emph{continuous-time} Markov chains, the time of the switches themselves is a continuous random variable, so times are in $\IR$)
\end{frame}

\begin{frame} 
Let $p_i(t)$ be the probability that state $S_i$ occurs on the $t^{th}$ time step, $1\leq i\leq n$
\vfill
One of the key assumptions in Markov chains is that the process is \defword{memoryless}: the transition that occurs from time $t$ to time $t+1$ depends only on the state of the system at time $t$
\vfill
Since one the states $S_i$ must occur on the $t^{th}$ time step,
\[
p_1(t)+p_2(t)+\cdots+p_n(t)=1
\]
\end{frame}

\begin{frame}{The transition matrix}
Let
\[
P=[p_{ij}]
\]
be the \defword{transition matrix} (or \defword{projection matrix}) of the Markov chain, where
\[
p_{ij} = \IP(S_i|S_j)
\]
\ie, the probability of making a transition from state $j$ to state $i$
\vfill
Many texts, \emph{e.g.}, \cite{KemenySnell1983}, define $p_{ij}=\IP(S_j|S_i)$. This works the same way, except it leads to $P^T$ instead of $P$ (or row vectors instead of column vectors)
\end{frame}

\begin{frame} 
Let $p_i(t+1)$ be the probability that state $S_i$, $1\leq i\leq n$, occurs on the $(t+1)^{th}$ step. 
There are $n$ ways to be in state $S_i$ at step $t+1$:
\begin{enumerate}
	\item Step $t$ is $S_1$. The probability of getting $S_1$ on the $t^{th}$ step is $p_1(t)$ and the probability of having $S_i$ after $S_1$ is $p_{i1}$, so $\IP(S_i|S_1)=p_{i1}p_1(k)$
	\item We get $S_2$ on step $t$ and $S_i$ on step $t+1$ and $\IP(S_i|S_2)=p_{i2}p_2(t)$
	\item[..]
	\item[n.] The probability of occurrence of $S_i$ at step $t+1$ if $S_n$ at step $t$ is $\IP(S_i|S_n)=p_{in}p_n(t)$
\end{enumerate}
\vfill
\begin{align*}
\implies p_i(t+1) &= \IP(S_i|S_1)+\cdots+\IP(S_i|S_n) \\
&= p_{i1}p_1(t)+\cdots+p_{in}p_n(t)
\end{align*}
\end{frame}

\begin{frame} 
Therefore,
\begin{align*}
p_1(t+1) &= p_{11}p_1(t)+p_{12}p_2(t)+\dots+p_{1n}p_n(t) \\
& \;\;\vdots\\
p_k(t+1) &= p_{k1}p_1(t)+p_{k2}p_2(t)+\dots+p_{kr}p_r(t) \\
& \;\;\vdots\\
p_n(t+1) &= p_{n1}p_1(t)+p_{n2}p_2(t)+\dots+p_{nn}p_r(t)
\end{align*}
\end{frame}

\begin{frame} 
In matrix form,
\begin{equation}\label{eq:CTMC_vector}
p(t+1)=Pp(t), \quad t=1,2,3,\dots
\end{equation}
where $p(t)=(p_1(t),p_{2}(t),\dots , p_n(t))^T$ is a (column) probability vector and $P=[p_{ij}]\in\M_n$ is the transition matrix
\begin{equation}\label{eq:DMTC_transition_matrix}
P=
\begin{pmatrix}
p_{11} & p_{12} & \cdots & p_{1n} \\
p_{21} & p_{22} & \cdots & p_{2n} \\
\vdots & \vdots && \vdots \\
p_{n1} & p_{n2} & \cdots & p_{nn}
\end{pmatrix}
\end{equation}
\end{frame}


\begin{frame}{Two observations about $P$}
\begin{itemize}
	\item Entries of $P$ being probabilities, they are all in $[0,1]$. So in particular, they are all nonnegative. We say $P$ is a \defword{nonnegative matrix} and write $P\geq 0$
	\vfill
	\item The \emph{column} sums of $P$ all equal 1. Take for instance the first column: its entries represent the probabilities of transition from state $1$. Since an event must always happen, the sum of these probabilities \emph{must} be 1. We say $P$ is a (column) \defword{stochastic matrix}
\end{itemize}
\end{frame}

\begin{frame} 
\begin{definition}[Stochastic matrix]
	The nonnegative $n\times n$ matrix $M$ is
	\begin{itemize}
		\item \defword{row stochastic} if $\sum_{j=1}^nm_{ij}=1$ for all $i=1,\dots, n$
		\item \defword{column stochastic} if $\sum_{i=1}^nm_{ij}=1$ for all $j=1,\dots, n$
		\item \defword{stochastic} if it is row or column stochastic
		\item \defword{doubly stochastic} if it is row \emph{and} column stochastic
	\end{itemize}
\end{definition}
\end{frame}


\begin{frame}{An interesting property of stochastic matrices}
\begin{theorem}\label{th:spectrum_stochastic_matrix}
	Let $M$ be a stochastic matrix. Then all eigenvalues $\lambda$ of $M$ are such that $|\lambda|\leq 1$. 
	Furthermore, $\lambda =1$ is an eigenvalue of $M$
\end{theorem}
%%
\vfill
%%
Assume w.l.o.g. that $M$ is row stochastic, \ie, $M$ has row sums 1. In vector form, $M\nbOne=\nbOne$. Now remember that $\lambda$ is an eigenvalue of $M$, with associated eigenvector $v\neq 0$, if and only if $Mv=\lambda v$. So, in the expression $M\nbOne=\nbOne$, we read an eigenvector, $\nbOne$, and an eigenvalue, $1$
\vfill
Proving the first conclusion of Theorem~\ref{th:spectrum_stochastic_matrix} involves a theorem called the Perron-Frobenius Theorem, which we will see in much detail later
\end{frame}

\begin{frame}{Long time behaviour}
Let $p_0:=p(0)=(p_1(0),\ldots,p_n(0))^T$ be the initial probability distribution vector, with $\nbOne^Tp_0=1$, \ie, such that the sum of the entries of $p_0$ be 1; we could also write $\langle p_0,\nbOne\rangle = 1$. Then
\begin{align*}
p(1) &= Pp(0) = Pp_0 \\
p(2) &= Pp(1)\\
&= P(Pp_0) \\
&= P ^2p_0
\end{align*}
Iterating, for any $t\in\IN$,
\[
p(t)=P^tp_0
\]
\vfill
Rings a bell? (cf Section~\ref{sec:linear_DE})
\end{frame}

\begin{frame}
So the long time evolution of the system is governed by
\begin{equation}\label{eq:limit_DTMC}
\lim_{t\rightarrow +\infty}p(t)
=\lim_{t\rightarrow +\infty}P^tp_0
=\left(\lim_{t\rightarrow +\infty}P^t\right)p_0
\end{equation}
if the latter limit exists
\vfill
So if we can characterize the nature of matrix $P^t$ and in particular, the existence of the limit $\lim_{t\to\infty}P^t$, we will know the long time behaviour of the Markov chain
\end{frame}

\begin{frame}
\begin{theorem}
If $M,N$ are nonsingular stochastic matrices, then $MN$ is a stochastic matrix
\end{theorem}
\vfill
\begin{corollary}
If $M$ is a nonsingular stochastic matrix, then for any $t\in\IN$, $M^t$ is a stochastic matrix
\end{corollary}
\vfill
$\implies$ matrix $P^t$ in \eqref{eq:limit_DTMC} is stochastic; so, in particular, it is a nonnegative matrix with column sums all equal to 1
\end{frame}


\begin{frame}{Regular Markov chains}
\begin{definition}[Regular Markov chain]
A \defword{regular Markov chain} is one in which $P^k$ is positive for some integer $k>0$, i.e., $P^k$ has only positive entries, no zero entries
\end{definition}
\vfill
\begin{definition}[Primitive matrix]
A nonnegative matrix $M$ is \defword{primitive} if, and only if, there is an integer $k>0$ such that $M^k$ is (entry-wise) positive
\end{definition}
\vfill
\begin{theorem}
A Markov chain is regular $\iff$ $P$ is primitive
\end{theorem}
\end{frame}

\begin{frame}{Regular Markov chains are well-behaved}
\begin{theorem}\label{th:behaviour_regular_MC}
If $P$ is the transition matrix of a regular Markov chain, then
\begin{enumerate}
\item the powers $P^t$ approach a stochastic matrix $W$
\item each column of $W$ is the same vector $w=(w_1,\ldots,w_n)^T$
\item the components of $w$ are positive
\end{enumerate}
\end{theorem}
\vfill
So if the Markov chain is regular, \eqref{eq:limit_DTMC} becomes
\[
\lim_{t\rightarrow +\infty}p(t)=\lim_{t\rightarrow +\infty}P^tp_0
=Wp_0
\]
\end{frame}

\begin{frame} 
Let $M\in\M_n$, $u,v$ be two column vectors, $\lambda\in\IC$. Then, if  
\[
Mu=\lambda u
\]
$u$ is the (right) eigenvector corresponding to $\lambda$, and if
\[
v^TM=\lambda v^T
\]
then $v$ is the left eigenvector corresponding to $\lambda$
\vfill 
To a given eigenvalue there corresponds one left and one right eigenvector (to multiples)
\vfill
$(v^TM)=(\lambda v^T)^T\iff M^Tv=\lambda v$, so if your numeric/symbolic solver spits out right eigenvectors, to get left ones, compute eigenvectors of $M^T$
\end{frame}

\begin{frame}{Back to the regular MC}
We already know that the left eigenvector corresponding to 1 is $\nbOne^T$, since $\nbOne^TP=\nbOne^T$, \ie, the column sums of $P$ all equal 1
\vfill
The vector $w$ in Theorem~\ref{th:behaviour_regular_MC} is the right eigenvector corresponding to the eigenvalue 1 of $P$
\vfill
To see this, remark that, if $p(t)$ converges, then $p(t+1)=Pp(t)$ in the limit for large $t$, so $w$ is a fixed point of the system. We thus write
\[
w=Pw
\]
and solve for $w$, which amounts to finding $w$ as the (right) eigenvector corresponding to the eigenvalue 1
\end{frame}

\begin{frame}
When you compute an eigenvector, the result is to a multiple and often the expression needs normalising (you want a probability vector)
\vfill
Once you obtain $w$, check that the norm 
\[
\|w\|=w_1+\cdots+w_n
\] 
is equal to one. If not, normalise as 
\[
\frac{w}{\|w\|}
\]
\end{frame}


\begin{frame}{Absorbing Markov chains}
Suppose now that the matrix is not only not primitive but also \defword{reducible}
\vfill
\begin{definition}[Reducible/irreducible matrices]
$0\leq M\in\M_n$ is \defword{reducible} if $\exists P\in\M_n$, permutation matrix, s.t.
\[
P^TMP =
\begin{pmatrix}
S & R \\ 0 & Q
\end{pmatrix}
\]
If no such matrix exists, $M$ is \defword{irreducible}
\end{definition}
\vfill
Let $\G(M)$ be the digraph induced by $M$
\begin{theorem}
$0\leq M\in\M_n$ irreducible $\iff$ $\G(M)$ strongly connected
\end{theorem}
\end{frame}

\begin{frame}
So if the transition matrix $P$ is reducible, $\G(P)$ is not strongly connected, \ie, there are states of the chain that are not accessible from others
\vfill
When in a state that does not have access to other states, we are stuck.. we say \defword{absorbed}
\vfill
An \defword{absorbing Markov chain} is one where at least one state is absorbing
\vfill
Theorem~\ref{th:behaviour_regular_MC} does not apply here, but we get a lot of interesting properties by observing that $P$ can be put in the form
\[
\begin{pmatrix}
\II& R \\ 0 & Q
\end{pmatrix}
\]
and considering the \defword{fundamental matrix} $N=(\II-Q)^{-1}$
\end{frame}

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discretisation of partial differential equations}
\label{sec:discretisation_PDEs}

\begin{frame}
Following \cite{Windisch1989}: $\Omega\subset\IR^d$ ($d\geq 1$) a bounded connected region, $u=u(x)$ for $x\in\Omega$; a linear elliptic BVP on $\Omega$ takes the form 
\begin{subequations}\label{sys:PDE}
\begin{align}
Lu &= f,\quad \Omega \\
\alpha u+\beta \frac{\partial u}{\partial v} &= g, \quad \partial\Omega
\end{align}
\end{subequations}
where $L$ is a linear differential operator of the form
\[
Lu = -\nabla(k\nabla u+bu)+qu
\]
Suppose that the scalar functions $k(x)$, $q(x)$ and the vector function
$b(x)=(b_1(x),\ldots,b_d(x))^T$ for $d>1$, otherwise a scalar function
$b(x)$, are sufficiently smooth over the region $\Omega$. 
Further, let $k(x)\geq k_0=\textrm{const} > 0$ and $q(x)\geq 0$ for each $x\in\Omega$
\vfill
$L$ together with corresponding BC on $\partial\Omega$ is a linear elliptic differential operator
\end{frame}


\begin{frame} 
As a particular example, suppose $\Omega=(0,1)$ is a one-dimensional domain, i.e., an interval, and suppose
\[
\bar{\omega}_h=
\left\{
0=x_1<x_2<\ldots<x_n=1
\right\}
=\omega_h+\gamma_h
\]
is a discretisation of the closure of said interval, with $\gamma_h=\{x_0,x_n\}$ and $h_i=h=1/n$ be a uniform grid, i.e., $x_i=ih$. Consider the following problem
\begin{subequations}
\begin{align}
Lu &= -u''+b(x)u'=0,\quad x\in\Omega \\
u(0) &= u_0,\quad u(1)=u_1
\end{align}
\end{subequations}
with $b(x)$ bounded in $\Omega$
\end{frame}

\begin{frame} 
Let us approximate on $\bar{\omega}$ using \emph{centred differences} (a finite difference scheme):
\begin{subequations}
\begin{align}
y_0 &= u_0 \\
-D_+D_-y_i +b_iD_0y_i &= 0, \quad i=1,\ldots,n-1 \\
y_n &= u_1
\end{align}
\end{subequations}
where $b_i=b(x_i)$. The notation $D_+D_-$ and $D_0$ refer to \emph{difference operators}:
\begin{subequations}
\begin{align}
D_+y_i &= \frac{y_{i+1}-y_i}{h} \\
D_-y_i &= \frac{y_{i}-y_{i-1}}{h} \\
D_0y_i &= \frac{y_{i+1}-y_{i-1}}{2h} \\
D_+D_-y_i &= \frac{y_{i-1}-2y_i+y_{i+1}}{h^2}
\end{align}
\end{subequations}
These operators ``encode'' derivatives on the discrete grid used
\end{frame}


\begin{frame} 
For $i=1,\ldots,n-1$, define $\gamma_i = {hb_i}/2$. 
Then the problem can be written in matrix form as
\begin{equation}
Ay=f
\end{equation}
where $f=(u_0,0,\ldots,0,u_1)^T=u_0e_1+u_1e_{n+1}$ and
{\footnotesize
\[
A=
\begin{pmatrix}
1 & 0 & \\
-(1+\gamma_1) & 2 & -(1-\gamma+1) & \\
& -(1+\gamma_2) & 2 & -(1-\gamma_2) & \\
& & & \\
& & \ddots & \ddots & \ddots \\
& & & \\
& & & -(1+\gamma_{n-1}) & 2 & -(1-\gamma_{n-1}) \\
& & & & 0 & 1
\end{pmatrix}
\]
}

Properties of the (approximate) solution then depend on the properties of the M-matrix $A$
\end{frame}






%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]
\frametitle{References}
\bibliographystyle{amsalpha}
\bibliography{MATH-4370-7370-lecture-notes.bib}
\end{frame}

\end{document}
